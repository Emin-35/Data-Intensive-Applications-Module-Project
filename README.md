# Hisse Senetlerinin SektÃ¶rel Benzerlik Analizi ve SÄ±nÄ±flandÄ±rma Modeli

**Bu proje, TÃ¼bitak'ta Senior Researcher, Quantitative Analyst olarak gÃ¶revde bulunan [Dr. Ä°smail GÃ¼zel](https://www.linkedin.com/in/ismail-g%C3%BCzel-phd-7b9935a2/?locale=tr_TR) mentorluÄŸunda, [Milli Teknoloji Akademisi](https://www.milliteknolojiakademisi.gov.tr/) kapsamÄ±nda aldÄ±ÄŸÄ±m "Veri YoÄŸun Uygulamalar" eÄŸitimi sonrasÄ±nda tamamlanan bir projedir.**

## Proje AmacÄ±
Bu projede, farklÄ± sektÃ¶rlerdeki hisse senetlerinin zaman serisi davranÄ±ÅŸlarÄ± analiz edilerek, bir hisse senedinin hangi sektÃ¶re daha Ã§ok benzediÄŸi tespit edilmiÅŸ ve bu bilgilerin yatÄ±rÄ±m stratejilerinde kullanÄ±labilirliÄŸi incelenmiÅŸtir.

## KullanÄ±lan AraÃ§lar ve Teknolojiler
- **Programlama Dili**: Python
- **Veri KaynaklarÄ±**: yfinance, finansal API'lar
- **KÃ¼tÃ¼phaneler**:
  - Veri Toplama: `yfinance`
  - Veri Ã–n Ä°ÅŸleme: `pandas`, `numpy`
  - Zaman Serisi Analizi: `tsfresh`
  - Makine Ã–ÄŸrenmesi: `scikit-learn`, `XGBoost`
  - Veri GÃ¶rselleÅŸtirme: `Matplotlib`, `BeautifulSoup`

## Proje AdÄ±mlarÄ±
1. **Veri Toplama**
   - `yfinance` API'leri kullanÄ±larak 2005'ten itibaren aylÄ±k getirilerle hisse senedi verileri toplandÄ±.
   - Web scraping ile sektÃ¶r ve hisse senedi listeleri Ã§ekildi.
```python
def fetch_sectors_names():
  url = "https://stockanalysis.com/stocks/industry/sectors/"
  response = requests.get(url)

  if response.status_code == 200:
    soup = BeautifulSoup(response.content, "html.parser")
    df=pd.read_html(StringIO(str(soup.find_all("table"))))[0]

  else:
    print(f"Error: Failed to fetch data from page {url}")
  return df

|--------------------------------------------|

def fetch_industry_names():
  url = "https://stockanalysis.com/stocks/industry/all/"
  response = requests.get(url)

  if response.status_code == 200:
    soup = BeautifulSoup(response.content, "html.parser")
    df=pd.read_html(StringIO(str(soup.find_all("table"))))[0]

  else:
    print(f"Error: Failed to fetch data from page {url}")
  return df

|--------------------------------------------|

def fetch_data(sectors):
  url = f"https://stockanalysis.com/stocks/sector/{sectors}/"
  response = requests.get(url)

  if response.status_code == 200:
    soup = BeautifulSoup(response.content, "html.parser")
    df=pd.read_html(StringIO(str(soup.find_all("table"))))[0]
    df.drop(columns='No.', inplace=True)

  else:
    print(f"Error: Failed to fetch data from page {url}")
  return df

|--------------------------------------------|

# Ã–rnek KullanÄ±m
fetch_data(sectors='energy').to_csv('../data/stock_sectors/energy.csv')
fetch_data(sectors='financials').to_csv('../data/stock_sectors/financials.csv')

```

2. **Veri Ã–n Ä°ÅŸleme**
   - Eksik veriler `forward-fill`, `backward-fill`, `ortalama` ve `medyan` ile dolduruldu.
```python
# GÃ¼nlÃ¼k aÃ§Ä±lÄ±ÅŸ deÄŸerleri, "Open" fiyatlarÄ±
data_open = final_data['Open']
# GÃ¼nlÃ¼k verileri aylÄ±ÄŸa Ã§evir
data_open_monthly = data_open.resample('M').first()

# Her ÅŸirket iÃ§in eksik verilerin ortalamasÄ±nÄ± bul
missing_ratio = data_open_monthly.isna().mean()
filtered_data = data_open_monthly.loc[:, missing_ratio < 0.4] # %40'tan fazla eksik veri olan sÃ¼tunlarÄ± at

# AylÄ±k aÃ§Ä±lÄ±ÅŸ fiyatlarÄ± benzerlik gÃ¶stereceÄŸi iÃ§in ffill ve bfill kullan
filtered_data = filtered_data.ffill().bfill()

```
| Veri DoldurmanÄ±n Ã–nemi                     | GÃ¶rsel |
|--------------------------------------------|--------|
| **TemizlenmemiÅŸ veri** | ![image](https://github.com/user-attachments/assets/1a8fd36e-121a-4256-a911-6087981155f2)|
| **TemizlenmiÅŸ ve DÃ¼zenlenmiÅŸ veri** |![image](https://github.com/user-attachments/assets/925fd0d6-e486-4d08-9c87-7f35c3f2826e)|

   - Zaman serisi duraÄŸan hale getirilmek iÃ§in log dÃ¶nÃ¼ÅŸÃ¼mÃ¼ ve fark alma (â€œdifferencingâ€) iÅŸlemleri uygulandÄ±.
```python
# HalihazÄ±rdaki Open sÃ¼tununu gÃ¼ncellemek yerine yeni sÃ¼tun aÃ§mak daha mantÄ±klÄ±
long_formatted_data["Open_LOG"] = np.log1p(long_formatted_data["Open"])
# Ã§Ã¼nkÃ¼ bu kÄ±smÄ± tekrar tekrar Ã§alÄ±ÅŸtÄ±rÄ±rsak Open deÄŸerleri sÃ¼rekli deÄŸiÅŸiyor olacak.
```
| Log1/y AlmanÄ±n Ã–nemi                     | GÃ¶rsel |
|--------------------------------------------|--------|
| **AykÄ±rÄ± ve uÃ§ deÄŸerler** | ![image](https://github.com/user-attachments/assets/a4569c32-8b8a-4679-84b8-c9c3c33bf4d9)|
| **DÃ¼zenlenmiÅŸ DeÄŸerler** |![image](https://github.com/user-attachments/assets/e6a5c547-355e-47ee-ad7c-609e5c3c9f74)|
    - **Open DeÄŸerleri**: Ä°lk histogramdaki x=0'daki Ã§ubuk muhtemelen veri kÃ¼mesinde Ã§ok sayÄ±da sÄ±fÄ±r veya Ã§ok kÃ¼Ã§Ã¼k deÄŸerler olduÄŸunu gÃ¶sterir.
      - Bu, 'Open' sÃ¼tununda Ã§ok sayÄ±da sÄ±fÄ±r veya sÄ±fÄ±ra yakÄ±n deÄŸer varsa meydana gelebilir. Ancak Ã¶nemli olan, x'teki diÄŸer sayÄ±larÄ±n Ã§ok bÃ¼yÃ¼k olmasÄ±dÄ±r.
    - **Open_LOG DeÄŸerleri**: Log dÃ¶nÃ¼ÅŸÃ¼mÃ¼ (log1p) deÄŸerleri daha eÅŸit bir ÅŸekilde daÄŸÄ±lmÄ±ÅŸtÄ±r ve aÅŸÄ±rÄ± uÃ§ deÄŸerlerin etkisini azaltÄ±lmÄ±ÅŸtÄ±r
  
   - Kategorik deÄŸiÅŸkenler `one-hot encoding` veya `label encoding` ile sayÄ±sal hale getirildi.


| Kategorik DeÄŸiÅŸkenler                     | GÃ¶rsel |
|--------------------------------------------|--------|
| **Encoding YapÄ±lmamÄ±ÅŸ Veri** | ![image](https://github.com/user-attachments/assets/fcbcfcdd-52e1-4871-972f-3a8f848e4e3b)|
| **Encoding YapÄ±lmÄ±ÅŸ Veri** |![Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2025-02-14 185538](https://github.com/user-attachments/assets/557b1026-3f8e-4792-9d05-0038ffae985d)|

3. **Ã–znitelik Ã‡Ä±karma ve SeÃ§me**
   - `tsfresh` kÃ¼tÃ¼phanesi ile istatistiksel Ã¶znitelikler (ortalama, standart sapma, otokorelasyon vb.) Ã§Ä±karÄ±ldÄ±.
```python
import tsfresh
from tsfresh.feature_extraction import EfficientFCParameters
# Extract features using only the 'Open' column
data_extract_features = tsfresh.extract_features(
  data_open_filled,
  column_id='Ticker',
  column_sort='Date',
  column_value='Open_LOG', # Explicitly specify the value column
  default_fc_parameters=EfficientFCParameters()
)

Index(['Open_LOG__variance_larger_than_standard_deviation',
 'Open_LOG__abs_energy', 'Open_LOG__mean_abs_change',
 'Open_LOG__mean_change',
 'Open_LOG__mean_second_derivative_central',
 'Open_LOG__median',
 ...
 'Open_LOG__fourier_entropy__bins_5',
 'Open_LOG__permutation_entropy__dimension_3__tau_1',
 'Open_LOG__permutation_entropy__dimension_4__tau_1',
 'Open_LOG__permutation_entropy__dimension_5__tau_1',
 'Open_LOG__query_similarity_count__query_None__threshold_0.0',
 'Open_LOG__mean_n_absolute_max__number_of_maxima_7'],
 dtype='object', length=777)

```
   - L1 regularization (Lasso), Recursive Feature Elimination (RFE) ve Principal Component Analysis (PCA) ile Ã¶znitelik seÃ§imi yapÄ±ldÄ±. En iyi sonucu veren seÃ§ildi. (RFE)
```python
# Model ve Veri Ä°ÅŸleme AraÃ§larÄ±
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
# SÄ±nÄ±flandÄ±rma Modelleri
from sklearn.ensemble import RandomForestClassifier,
# Ã–zellik SeÃ§imi ve Boyut Ä°ndirgeme
from sklearn.feature_selection import RFE

# Veriyi yÃ¼kle
extracted_labeled_data = pd.read_csv('../data/processed_data/extracted_labeled_features.xlsx')

# Train ve Test iÃ§in sÃ¼tunlarÄ± seÃ§
X = extracted_labeled_data.drop(columns=['Label'])
y = extracted_labeled_data['Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

### 1. Recursive Feature Elimination (RFE) ###
selector_rfe = RFE(RandomForestClassifier(random_state=10))
selector_rfe.fit(X_train, y_train)

# Transform datasets
X_train_rfe = selector_rfe.transform(X_train)
X_test_rfe = selector_rfe.transform(X_test)

# SeÃ§ilen feature'larÄ± yazdÄ±r
selected_features_rfe = X_train.columns[selector_rfe.get_support()]
print(f"RFE Selected Features ({len(selected_features_rfe)}): {list(selected_features_rfe)}")

# RFE Selected Features (385)

# Bu feature'larÄ± kullanarak yeni veri setleri oluÅŸtur (385 yerine en iyi 20 feature seÃ§ilmiÅŸtir!)
X_train_top20_rf = X_train[top_20_features_rf]
X_test_top20_rf = X_test[top_20_features_rf]

# Model oluÅŸtur ve eÄŸit
rf_model_top20 = RandomForestClassifier(random_state=10)
rf_model_top20.fit(X_train_top20_rf, y_train)

# Test seti Ã¼zerinde tahmin yap
y_pred_top20_rf = rf_model_top20.predict(X_test_top20_rf)

# DoÄŸruluk (accuracy) hesapla RFE
accuracy_top20_rf = accuracy_score(y_test, y_pred_top20_rf)
print(f"\nEn Ã¶nemli 20 feature kullanÄ±larak elde edilen doÄŸruluk: {accuracy_top20_rf:.4f}")

# Ã‡apraz doÄŸrulama yap RFE
cv_scores_top20_rf = cross_val_score(rf_model_top20, X_train_top20_rf, y_train, cv=5, scoring='accuracy')
print(f"Ã‡apraz doÄŸrulama skorlarÄ±: {cv_scores_top20_rf}")
print(f"Ortalama Ã§apraz doÄŸrulama skoru: {cv_scores_top20_rf.mean():.4f}")
```
![image](https://github.com/user-attachments/assets/750838e5-85b1-463d-936a-04fa6934da0a)

4. **Model GeliÅŸtirme**
   - `Random Forest`, `Gradient Boosting`, `XGBoost` ve `CatBoost` algoritmalarÄ± ile modeller eÄŸitildi.
```python
# Gerekli kÃ¼tÃ¼phaneleri iÃ§e aktaralÄ±m
from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import RidgeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

# Label encoding (eÄŸer label'lar string ise)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# FarklÄ± modelleri tanÄ±mla
models = {
  # Temel modeller
  "Logistic Regression": LogisticRegression(max_iter=1000,random_state=0),
  "SVC": SVC(random_state=0, probability=True),
  "Random Forest": RandomForestClassifier(n_estimators=100,random_state=10),
  "Gradient Boosting": GradientBoostingClassifier(n_estimators=100,random_state=10),

  # GeliÅŸmiÅŸ modeller
  "XGBoost": XGBClassifier(n_estimators=100, random_state=10,use_label_encoder=False, eval_metric='logloss'),
  "MLP Classifier": MLPClassifier(hidden_layer_sizes=(100, 50),max_iter=1000, random_state=0),

  # Ek modeller
  "AdaBoost": AdaBoostClassifier(n_estimators=100, random_state=10),
  "Ridge Classifier": RidgeClassifier(random_state=0),
  "KNeighbors": KNeighborsClassifier(n_neighbors=5),
  "Gaussian NB": GaussianNB()
}

import os
import joblib # Modeli yÃ¼klemek iÃ§in

# KayÄ±t dizinini oluÅŸtur (eÄŸer yoksa)
save_dir = "../trained_models/"
os.makedirs(save_dir, exist_ok=True)

# SonuÃ§larÄ± tutacak liste
results = []

# Her modeli eÄŸit, test et ve accuracy score hesapla
for model_name, model in models.items():
  print(f"\n{model_name} eÄŸitiliyor...")
  model.fit(X_train_rfe, y_train_encoded) # Modeli eÄŸit
  y_pred = model.predict(X_test_rfe) # Test verisiyle tahmin yap

  # Model dosya adÄ±nÄ± oluÅŸtur ve kaydet
  model_filename = os.path.join(save_dir, f"{model_name.replace(' ','_')}.pkl")
  joblib.dump(model, model_filename) # Modeli kaydet
  print(f"{model_name} kaydedildi: {model_filename}")

  acc = accuracy_score(y_test_encoded, y_pred) # Accuracy hesapla
  print(f"{model_name} Accuracy: {acc:.4f}")

  target_names = [str(cls) for cls in label_encoder.classes_]
  # Classification report yazdÄ±r
  print(classification_report(y_test_encoded, y_pred,target_names=target_names))

  # SonuÃ§larÄ± sakla
  results.append((model_name, acc))

# En iyi modeli bul
best_model = max(results, key=lambda x: x[1])
print(f"\nEn iyi model: {best_model[0]} (Accuracy: {best_model[1]:.4f})")
```
| Logistic Regression | SVC | Random Forest | Gradient Boosting | XGBoost |
|----------|----------|----------|----------|----------|
| ![image](https://github.com/user-attachments/assets/d305c903-52f3-4fb2-b899-dc95408d7d75)| ![image](https://github.com/user-attachments/assets/33d32ccb-7a7d-471a-86bb-5b92f5568bb4)|![image](https://github.com/user-attachments/assets/378ad18d-70dd-45f0-9302-d3955d5e3095)|![image](https://github.com/user-attachments/assets/e1e24265-7ad4-430d-8a20-f49f3b10bf9e)|![image](https://github.com/user-attachments/assets/dca0f8d1-c7b8-4367-8f44-17e6df35ae86)|
| MLP Classifier | AdaBoost | Ridge Classifier | KNeighbors | Gaussian NB |
| ![image](https://github.com/user-attachments/assets/94acd541-7c22-466b-8a07-b843ca0153a9)| ![image](https://github.com/user-attachments/assets/539f6866-131f-41cd-83e5-29b6da3133d2)| ![image](https://github.com/user-attachments/assets/427a7c92-e183-4a79-a724-1d2d4c55cb89)|![image](https://github.com/user-attachments/assets/61c01d24-d504-40c2-b93a-e65a9e972796)|![image](https://github.com/user-attachments/assets/0970c2bb-e2d4-4c20-924a-04e5ab342548)|

## Model Performances

| Rank | Model                | Test Accuracy |
|------|----------------------|--------------|
| 1ï¸âƒ£  | **Gradient Boosting** | **0.816667** |
| 2ï¸âƒ£  | **XGBoost**           | **0.805556** |
| 3ï¸âƒ£  | **Ridge Classifier**  | **0.794444** |
| 4ï¸âƒ£  | Random Forest        | 0.755556     |
| 5ï¸âƒ£  | AdaBoost            | 0.727778     |
| 6ï¸âƒ£  | Logistic Regression | 0.711111     |
| 7ï¸âƒ£  | Gaussian NB         | 0.600000     |
| 8ï¸âƒ£  | SVC                 | 0.594444     |
| 9ï¸âƒ£  | MLP Classifier      | 0.561111     |
| ğŸ”Ÿ  | KNeighbors          | 0.483333     |



   - `Grid Search` ve `Bayesian Optimization` kullanÄ±larak hiperparametre optimizasyonu yapÄ±ldÄ±.
```python
from skopt import BayesSearchCV
from skopt.space import Real, Integer

# Model
gb_model = GradientBoostingClassifier()
# Hiperparametre aralÄ±ÄŸÄ± (Bayesian Search)
param_space = {
  'n_estimators': Integer(50, 500), # AÄŸaÃ§ sayÄ±sÄ±
  'learning_rate': Real(0.01, 0.2, prior='log-uniform'), # Ã–ÄŸrenme oranÄ±
  'max_depth': Integer(3, 10), # AÄŸaÃ§ derinliÄŸi
  'subsample': Real(0.5, 1.0), # Rastgele Ã¶rnekleme oranÄ±
}
# Bayesian Search
bayes_search = BayesSearchCV(
  gb_model,
  param_space,
  n_iter=32, # KaÃ§ farklÄ± kombinasyon denenecek
  cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
  scoring='accuracy',
  n_jobs=-1,
  verbose=2
)

# Modeli eÄŸit
bayes_search.fit(X_train_rfe, y_train_encoded)

# En iyi modeli kaydet
best_model = bayes_search.best_estimator_joblib.dump(best_model,"../trained_models/Optimized_Gradient_Boosting.pkl")
```
   - `Cross-validation` ile modelin genelleme performansÄ± deÄŸerlendirildi.
```python
# En iyi modeli yÃ¼kle
best_model = joblib.load("../trained_models/Optimized_Gradient_Boosting.pkl")

# Cross-Validation ile performansÄ± Ã¶lÃ§
cv_scores = cross_val_score(best_model, X_train_rfe, y_train_encoded,cv=5, scoring='accuracy')

print("Cross-validation skorlarÄ±:", cv_scores)
print("Ortalama doÄŸruluk:", cv_scores.mean())
```
   - En iyi sonuÃ§ verilen model seÃ§ildi

5. **Model DeÄŸerlendirme**
   - Model performansÄ± `accuracy`, `F1-score`, `ROC-AUC` metrikleriyle deÄŸerlendirildi.

6. **SektÃ¶rel Benzerlik Analizi**
   - Hisse senetlerinin sektÃ¶rel benzerlikleri analiz edildi.
     - Bunun iÃ§in ilk Ã¶nce seÃ§ilen veri, modele uygun hale getirildi
```python
# --- 1. Veri Ä°ÅŸleme FonksiyonlarÄ± ---
def preprocess_open_values(df):
  """ Open deÄŸerlerini iÅŸleyip log dÃ¶nÃ¼ÅŸÃ¼mÃ¼ uygular ve 2005'ten itibaren filtreler. """
  df_data = yf.download(df, start='2005-01-01')

  df_open_data = df_data['Open'] # Sadece open verisini al
  df_open_data = df_open_data.ffill().fillna(0) # Ä°lk ffill, sonra bÃ¼tÃ¼n boÅŸ deÄŸerleri 0 ile doldur
  df_open_data = df_open_data.resample('M').first() # GÃ¼nlÃ¼k verileri aylÄ±ÄŸa Ã§evir.

  df_open_data_long = df_open_data.reset_index().melt(id_vars='Date', var_name='Ticker', value_name='Open') # Long tÃ¼rÃ¼ne Ã§evir
  df_open_data_long['Open_LOG'] = np.log1p(df_open_data_long['Open']) # Open deÄŸerlerinin, Log1/y deÄŸerini al

  return df_open_data_long[['Date', 'Ticker', 'Open_LOG']]

# --- 2. Ã–zellik Ã‡Ä±karma Fonksiyonu ---
def extract_features(df):
  """ Open_LOG Ã¼zerinden Ã¶zellik Ã§Ä±karÄ±r ve seÃ§ili olanlarÄ± dÃ¶ndÃ¼rÃ¼r. """
  features = tsfresh.extract_features(
    df,
    column_id='Ticker',
    column_sort='Date',
    column_value='Open_LOG',
    default_fc_parameters=EfficientFCParameters()
  )
  return features[selected_features_rfe]

# --- 3. Pipeline TanÄ±mlama ---
data_pipeline = Pipeline([
  ('preprocessing', FunctionTransformer(preprocess_open_values)), # Open iÅŸlemleri
  ('feature_extraction', FunctionTransformer(extract_features)), # Tsfresh Ã¶zellik Ã§Ä±karÄ±mÄ±
])

# --- 4. Model YÃ¼kleme ---
#mlp_model = joblib.load("../trained_models/Gradient_Boosting.pkl") # EÄŸittiÄŸin MLP modelini yÃ¼kle
optimized_gradient_boost_model = joblib.load("../trained_models/Optimized_Gradient_Boosting.pkl")

def predict_sector(df):
  """ Ä°ÅŸlenmiÅŸ veriyi modele sokar ve sektÃ¶re olan olasÄ±lÄ±klarÄ±nÄ± dÃ¶ndÃ¼rÃ¼r. """
  X_processed = data_pipeline.transform(df)

  probabilities = optimized_gradient_boost_model.predict_proba(X_processed)[0] # OlasÄ±lÄ±klarÄ± al
  sector_mapping = {0: 'Financials', 1: 'Healthcare', 2: 'Technology'}

  # YuvarlanmÄ±ÅŸ olasÄ±lÄ±klarÄ± bir sÃ¶zlÃ¼k olarak dÃ¶ndÃ¼r
  sector_probabilities = {sector_mapping[i]: round(prob, 4) for i, prob in enumerate(probabilities)}
  return sector_probabilities

```
   - Ã–rneÄŸin, Real-Estate sektÃ¶rÃ¼ndeki hisselerin hangi sektÃ¶rlere benzediÄŸi incelendi.
   
| Veri Tahmini TÃ¼rÃ¼                          | GÃ¶rsel |
|--------------------------------------------|--------|
| **Verisetinde olmayan bir endÃ¼striden, ÅŸirket veri benzerliÄŸi** | ![image](https://github.com/user-attachments/assets/f0fe0e5e-a182-4f93-9a7c-c3b2d4be62a5) |
| **Verisetinde olan bir endÃ¼striden, model iÃ§inde olmayan bir ÅŸirketin veri benzerliÄŸi** | ![image](https://github.com/user-attachments/assets/c3d35f6a-9fdc-4ed6-bd7a-5f77a720425f) |

7. **SonuÃ§larÄ±n GÃ¶rselleÅŸtirilmesi ve Raporlama**
   - Zaman serisi grafikleri, sektÃ¶rel benzerlik matrisleri ve model performans metrikleri gÃ¶rselleÅŸtirildi.
   - DetaylÄ± bir proje raporu hazÄ±rlandÄ±.

## Beklenen Ã‡Ä±ktÄ±lar
- **SektÃ¶rel benzerlikleri gÃ¶steren bir sÄ±nÄ±flandÄ±rma modeli**
- **FarklÄ± sektÃ¶rlerdeki hisse senetlerinin davranÄ±ÅŸlarÄ±nÄ± analiz eden bir rapor**
- **YatÄ±rÄ±m stratejileri iÃ§in sektÃ¶rel benzerlik bilgisi**

## Kurulum ve KullanÄ±m
Bu projeyi Ã§alÄ±ÅŸtÄ±rmak iÃ§in:
1. Gerekli kÃ¼tÃ¼phaneleri yÃ¼kleyin:
   ```bash
   pip install yfinance tsfresh scikit-learn scikit-optimize XGBoost matplotlib
   ```
2. Verileri toplayÄ±n ve Ã¶n iÅŸleyin.
3. Modeli eÄŸitin ve deÄŸerlendirin.
4. SonuÃ§larÄ± gÃ¶rselleÅŸtirin ve raporlayÄ±n.

## Lisans
Bu proje, Milli Teknoloji Akademisi kapsamÄ±nda gerÃ§ekleÅŸtirilmiÅŸ olup akademik ve eÄŸitim amacÄ±yla kullanÄ±labilir.
